{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, datetime, getopt, glob, itertools, logging, os, sys, time\n",
    "import helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyshark\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "devices_devicenames = ['August1', 'August2', 'Door1', 'Door2', 'Energy1', \n",
    "                       'Energy2', 'Kevo', 'Push', 'Room1', 'Room2', 'Weather']\n",
    "devices_publicaddrs = ['Home1', 'Home2']\n",
    "\n",
    "id_devicenames = [['Kevo','Unikey'],\n",
    "                'Eve Door 91B3',\n",
    "                'Eve Door DC42',\n",
    "                's',\n",
    "                'Aug',\n",
    "                'L402EL4',\n",
    "                'Eve Energy 51C0',\n",
    "                'Eve Energy 556E',\n",
    "                'Eve Weather 943D',\n",
    "                'Eve Room 8F24',\n",
    "                'Eve Room 4A04']\n",
    "\n",
    "BLE_DEVICES = sorted(devices_devicenames + devices_publicaddrs)\n",
    "\n",
    "# Devices that can be identified using public (static) advertising addresses\n",
    "DEVICES_PUBLICADDRS = {'ec:fe:7e:14:44:be' : 'Home1', \n",
    "                       'ec:fe:7e:14:44:a1' : 'Home2'}\n",
    "\n",
    "# Devices that can be identified using device names\n",
    "DEVICES_NAMES = {'August1': 'L402EL4',\n",
    "                'August2': 'Aug',\n",
    "                'Door1': 'Eve Door 91B3',\n",
    "                'Door2': 'Eve Door DC42',\n",
    "                'Energy1': 'Eve Energy 556E',\n",
    "                'Energy2': 'Eve Energy 51C0',\n",
    "                'Kevo': ['Kevo', 'Unikey'],\n",
    "                'Push': 's',\n",
    "                'Room1': 'Eve Room 4A04',\n",
    "                'Room2': 'Eve Room 8F24',\n",
    "                'Weather': 'Eve Weather 943D'}\n",
    "\n",
    "# Just the reverse of DEVICES_NAMES\n",
    "NAMES_DEVICES = {'Aug': 'August2',\n",
    "                 'Eve Door 91B3': 'Door1',\n",
    "                 'Eve Door DC42': 'Door2',\n",
    "                 'Eve Energy 51C0': 'Energy2',\n",
    "                 'Eve Energy 556E': 'Energy1',\n",
    "                 'Eve Room 4A04': 'Room1',\n",
    "                 'Eve Room 8F24': 'Room2',\n",
    "                 'Eve Weather 943D': 'Weather',\n",
    "                 'Kevo': 'Kevo',\n",
    "                 'L402EL4': 'August1',\n",
    "                 'Unikey': 'Kevo',\n",
    "                 's': 'Push'}\n",
    "\n",
    "DEVICE_TYPE = {'August1': 'lock',\n",
    "                'August2': 'lock',\n",
    "                'Door1': 'door',\n",
    "                'Door2': 'door',\n",
    "                'Energy1': 'plug',\n",
    "                'Energy2': 'plug',\n",
    "                'Home1': 'door',\n",
    "                'Home2': 'door',\n",
    "                'Kevo': 'lock',\n",
    "                'Push': 'temp',\n",
    "                'Room1': 'temp',\n",
    "                'Room2': 'temp',\n",
    "                'Weather': 'temp'}\n",
    "\n",
    "TRAINING_TEST = {'August1': 'train',\n",
    "                 'August2': 'test',\n",
    "                 'Door1': 'train',\n",
    "                 'Door2': 'test',\n",
    "                 'Energy1': 'train',\n",
    "                 'Energy2': 'train',\n",
    "                 'Home1': 'train',\n",
    "                 'Home2': 'train',\n",
    "                 'Kevo': 'train',\n",
    "                 'Push': 'train',\n",
    "                 'Room1': 'train',\n",
    "                 'Room2': 'test',\n",
    "                 'Weather': 'train'}\n",
    "\n",
    "PDU_TYPES = {0: 'ADV_IND',\n",
    "             1: 'ADV_DIRECT_IND',\n",
    "             2: 'ADV_NONCONN_IND',\n",
    "             3: 'SCAN_REQ',\n",
    "             4: 'SCAN_RSP',\n",
    "             5: 'CONNECT_REQ',\n",
    "             6: 'ADV_SCAN_IND'}\n",
    "\n",
    "SRC_DIR = './BLE_Source/'\n",
    "DST_DIR = './BLE_Destination/'\n",
    "PCAP_DIR = '/root/Documents/Thesis/BLE_PCAPS/'\n",
    "TIMING_PKT_NUMBER = 25000\n",
    "\n",
    "FEATURES = ['Name', 'DeviceName', 'AccessAddr', 'AdvertAddr', 'BLE_LL_Length', \n",
    "            'PDUTypeNum', 'TxAddr', 'CompanyID','ScanAddr',\n",
    "            'RFChannel', 'PacketLength', 'Time']\n",
    "\n",
    "path_name = os.getcwd()\n",
    "DATE = path_name[path_name.rindex('/')+1:]\n",
    "PROC_TIME = \"ble_processing_time_\" + DATE + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_packet(pkt, tgt_files_by_src):\n",
    "    \"\"\"\n",
    "    Parses a given packet and extracts the following features:\n",
    "        (BLE LL)\n",
    "        - device name\n",
    "        - access address\n",
    "        - advertising address\n",
    "        - BLE LL packet length (bytes)\n",
    "        - PDU type\n",
    "        - Tx address type (public or random)\n",
    "        - company id\n",
    "        - scanning address (if SCAN_REQ pdu_type)\n",
    "        \n",
    "        (BLE RF)\n",
    "        - rf channel (same as advertising channel: RF 0 = ADV 37, 12 = 38, 39 = 39)\n",
    "        \n",
    "        (Frame)\n",
    "        - total frame length (bytes)\n",
    "        - epoch time (seconds)      \n",
    "        \n",
    "    The features of the packet are written out to a csv row, which is\n",
    "    in turn written out to a csv file in the given dictionaries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pkt: (Pyshark packet object) the packet from which features will be extracted\n",
    "    tgt_files_by_src: (dictionary) a dictionary of open csv files.\n",
    "        The keys are device source addresses, and the values are the open csv files.\n",
    "    tgt_files_by_dst: (dictionary) a dictionary of open csv files.\n",
    "        The keys are device destination addresses, and the values are the open csv files.\n",
    "    \"\"\"\n",
    "    \n",
    "    public_addrs = DEVICES_PUBLICADDRS.keys()\n",
    "    known_names = NAMES_DEVICES.keys()\n",
    "    \n",
    "    try:        \n",
    "        # Find devices with known advertising addresses or device_names\n",
    "        advAddr = pkt.btle.get_field_value('advertising_address')        \n",
    "        name = pkt.btle.get_field_value('btcommon_eir_ad_entry_device_name')\n",
    "        \n",
    "        # Assign an identifier based on whether a known advAddr or name was found\n",
    "        identifier, identifier_type = (advAddr,'advAddr') if name == None else (name,'name')       \n",
    "        \n",
    "        if (identifier in public_addrs) or (identifier in known_names):\n",
    "                       \n",
    "            # BLE LL features\n",
    "            deviceName = pkt.btle.get_field_value('btcommon_eir_ad_entry_device_name')\n",
    "            accessAddr = pkt.btle.get_field_value('access_address')\n",
    "            advAddr = pkt.btle.get_field_value('advertising_address')\n",
    "            bleLength = pkt.btle.get_field_value('length')\n",
    "            pduType = pkt.btle.get_field_value('advertising_header_pdu_type')\n",
    "            txAddr = pkt.btle.get_field_value('advertising_header_randomized_tx')\n",
    "            companyID = pkt.btle.get_field_value('btcommon_eir_ad_entry_company_id')\n",
    "            scanAddr = pkt.btle.get_field_value('scanning_address')\n",
    "            \n",
    "            # BLE RF\n",
    "            rfChannel = pkt.btle_rf.get_field_value('channel')\n",
    "            \n",
    "            # Bluetooth\n",
    "            pktLength = pkt.frame_info.get_field_value('len')\n",
    "            epochTime = pkt.frame_info.get_field_value('time_epoch')       \n",
    "            \n",
    "            # Name as used in thesis document\n",
    "            name = DEVICES_PUBLICADDRS[identifier] if identifier_type == 'advAddr' else NAMES_DEVICES[identifier]\n",
    "                        \n",
    "            # Output matches the order of FEATURES\n",
    "            output = [name, deviceName, accessAddr, advAddr, bleLength, pduType, txAddr, companyID, scanAddr,\n",
    "                      rfChannel,\n",
    "                      pktLength, epochTime]\n",
    "            \n",
    "            # Write features to csv           \n",
    "            csv.writer(tgt_files_by_src[name]).writerow(output)\n",
    "           \n",
    "    \n",
    "    except AttributeError:\n",
    "        print \"ignored: \", pkt.number            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ble_extract_packet_features(filename = os.path.join(PCAP_DIR, 'master.pcap'), create_master=True):\n",
    "    \"\"\"\n",
    "    Unit that extracts wanted features out of packets in a packet capture file.\n",
    "    The feature_extractor focuses on features derived from packet information. \n",
    "    Secondary features are processed by the make_dataframe function.\n",
    "    Produces two csv files for each device in WIFI_DEVICES (see Global Variables).\n",
    "    One file is for all packets where the device is the source; the other is where the device is the destination.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: (string) the absolute path of the packet capture file\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    Source directory: (filesystem) creates a directory containing csv files for each device \n",
    "        where it is the source of the packet\n",
    "    Destination directory: (filesystem) creates a directory containing csv files for each device \n",
    "        where it is the destination of the packet\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    none\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare writers\n",
    "    pt_file = open(PROC_TIME, 'w')\n",
    "    csv.writer(pt_file).writerow([\"Unit\", \"Total Packets Processed\", \"Total Process Time\", \"Average Process Time\"])\n",
    "    pt_file.close()\n",
    "\n",
    "    # Initialize counters\n",
    "    pkt_count = 0\n",
    "    total_time_processing = 0\n",
    "    total_time_start = time.time()\n",
    "\n",
    "    # Initialize dicts for each device\n",
    "    tgt_files_by_src = {}\n",
    "    \n",
    "    # Combine all pcaps in directory in one master pcap\n",
    "    if (create_master):\n",
    "        try:\n",
    "            if os.path.exists(\"/root/Documents/Thesis/BLE_PCAPS/master.pcap\"):\n",
    "                os.remove(\"/root/Documents/Thesis/BLE_PCAPS/master.pcap\")\n",
    "                \n",
    "            ret = os.system('mergecap /root/Documents/Thesis/BLE_PCAPS/*.pcap -w \n",
    "                            /root/Documents/Thesis/BLE_PCAPS/master.pcap')\n",
    "            if ret != 0:\n",
    "                raise OSError\n",
    "        except OSError:\n",
    "            print 'Could not make master capture file'\n",
    "\n",
    "    # Initialize capture file \n",
    "    cap = pyshark.FileCapture(filename, only_summaries=False)\n",
    "\n",
    "    # Get time of first packet\n",
    "    prev_pkt_time = cap[0].frame_info.time_epoch\n",
    "\n",
    "    # Initialize output folders\n",
    "    helpers.init_dirs('ble')\n",
    "    \n",
    "    # Open output files for each Wi-Fi device\n",
    "    for device in BLE_DEVICES:\n",
    "        tgt_files_by_src[device] = open(SRC_DIR + device + \".csv\", 'a')\n",
    "        \n",
    "        # Initialize with column headers\n",
    "        csv.writer(tgt_files_by_src[device]).writerow(FEATURES)\n",
    "    \n",
    "    # Go through each packet in capture, and store pertinent packets to csv files\n",
    "    for pkt in cap:\n",
    "        \n",
    "        if pkt_count % TIMING_PKT_NUMBER == 0:\n",
    "            print \"Working packet #\", pkt_count, \"...\"\n",
    "        pkt_count += 1\n",
    "\n",
    "        time_start_singlepacket = time.time()\n",
    "        parse_packet(pkt, tgt_files_by_src)\n",
    "        total_time_processing += time.time() - time_start_singlepacket\n",
    "\n",
    "    total_time_elapsed = time.time() - total_time_start\n",
    "    \n",
    "    # Close files\n",
    "    for open_file in tgt_files_by_src.values():\n",
    "        open_file.close()\n",
    "        \n",
    "    # Calculate time variables\n",
    "    final_time = time.time()\n",
    "    normalized_total_time = (TIMING_PKT_NUMBER * total_time_elapsed) / pkt_count\n",
    "    normalized_processing_time = (TIMING_PKT_NUMBER * total_time_processing) / pkt_count\n",
    "\n",
    "    # Print time variables\n",
    "    print \"Total number of packets processed: \", pkt_count\n",
    "    print \"Total data processing time: \", total_time_elapsed\n",
    "    print \"Normalized total processing time per 25k packets: \", normalized_total_time\n",
    "    print \"Total capture file processing time: \", total_time_processing\n",
    "    print \"Normalized capture file processing time: \", normalized_processing_time\n",
    "\n",
    "    # Print out time metrics to csv\n",
    "    pt_file = open(PROC_TIME, 'a')\n",
    "    csv.writer(pt_file).writerow([\"Packet capture iteration\", pkt_count, \n",
    "                                  total_time_processing, normalized_processing_time])\n",
    "    csv.writer(pt_file).writerow([\"Component start and finish time\", total_time_start, \n",
    "                                  final_time, final_time-total_time_start])\n",
    "    pt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_assoc_pkts(df, device):\n",
    "    \"\"\"\n",
    "    Gets the count of packets of a given device that are sent within a second of each other (associated packets)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: (dataframe) the dataframe containing the packet information\n",
    "    device: (string) the name of the device for which the assoc_pkt count will be calculated\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    assoc_count: (pandas series) contains the assoc_packet count for each packet. \n",
    "                Uses the index of the packet from the dataframe\n",
    "    \"\"\"\n",
    "        \n",
    "    ASSOC_PKT_THRESHOLD = 1 # the threshold in seconds within which a packet will be considered an assoc_pkt\n",
    "\n",
    "    # Extract time values of all packets belonging to a certain device\n",
    "    df_device = df[df[\"Name\"]==device]\n",
    "    pkt_time_values = np.array(df_device[\"Time\"].values)\n",
    "    \n",
    "    assoc_pkt_counts = []\n",
    "    \n",
    "    # Iterate through each packet of the device\n",
    "    for pkt_index in range(0,len(df_device)):  \n",
    "\n",
    "        # Create an array of size=len(pkt_time_values) that contains the time value of packet X\n",
    "        pkt_time = np.full((len(pkt_time_values),),df_device.iloc[pkt_index][\"Time\"])\n",
    "\n",
    "        # Calculate the time difference between packet X and all other packets\n",
    "        diff = np.abs(np.subtract(pkt_time, pkt_time_values))\n",
    "\n",
    "        # Calculate the count of packets that would be considered an assoc_pkt based on ASSOC_PKT_THRESHOLD\n",
    "        assoc_pkts = (diff <= ASSOC_PKT_THRESHOLD).sum()\n",
    "        assoc_pkt_counts.append(assoc_pkts)\n",
    "        \n",
    "    \n",
    "    assoc_count = pd.Series(assoc_pkt_counts, index=df_device.index)\n",
    "    return assoc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(path='/root/Documents/Thesis/Code/BLE_Source'):\n",
    "    \"\"\"\n",
    "    Unit that takes all the csv files produced by the feature_extractor unit \n",
    "    and puts them into a pandas dataframe.\n",
    "    Returns a clean dataframe with all good data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: (filesystem) the absolute path of the folder containing the csv files\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    none\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe: (pandas dataframe) a useful data structure for machine learning\n",
    "    counts: (pandas series) packet counts for each device \n",
    "    \"\"\"\n",
    "    \n",
    "    # Search the path for csv files\n",
    "    all_csvs = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "    # Collect all csvs in one dataframe\n",
    "    df_from_each_file = (pd.read_csv(f) for f in all_csvs)\n",
    "    df = pd.concat(df_from_each_file, ignore_index=True, sort=False)\n",
    "\n",
    "    # Add device type of each packet\n",
    "    df[\"DeviceType\"] = df[\"Name\"].map(DEVICE_TYPE)\n",
    "        \n",
    "    # Add whether device is a training or test device\n",
    "    df[\"Set\"] = df[\"Name\"].map(TRAINING_TEST)\n",
    "    \n",
    "    # One-hot encode device type (response variable)\n",
    "    deviceType_series = pd.get_dummies(df[\"DeviceType\"])\n",
    "    df = pd.concat([df, deviceType_series], axis=1)\n",
    "    \n",
    "    # TODO: One-hot encode company ID \n",
    "    \n",
    "    # TODO: One-hot encode access address\n",
    "    \n",
    "    # TODO: One-hot encode adv address\n",
    "    \n",
    "    # TODO: One-hot encode scanning address\n",
    "    \n",
    "    # One-hot encode PDU_type\n",
    "    df[\"PDUType\"] = df[\"PDUTypeNum\"].map(PDU_TYPES)\n",
    "    pduType_series = pd.get_dummies(df[\"PDUType\"])\n",
    "    df = pd.concat([df, pduType_series], axis=1)\n",
    "    \n",
    "    # Get number of associated packets for each packet\n",
    "    list_assoc_pkts = []\n",
    "#     for device in list(df[\"Name\"].unique()):\n",
    "    for device in BLE_DEVICES:\n",
    "        assoc_pkts = count_assoc_pkts(df, device)\n",
    "        list_assoc_pkts.append(assoc_pkts)\n",
    "    df[\"Assoc_Packets\"] = pd.concat(list_assoc_pkts)\n",
    "    \n",
    "    # Fill NaNs with 0\n",
    "    df[\"CompanyID\"] = df[\"CompanyID\"].fillna(0)\n",
    "    df[\"ScanAddr\"] = df[\"ScanAddr\"].fillna(0)\n",
    "    \n",
    "    # Count packets for each device\n",
    "    device_counts = df[\"Name\"].value_counts()\n",
    "    print device_counts\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(X_train, y_train, X_test, y_test):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    randomforest = RandomForestClassifier(random_state=0, n_jobs=2)\n",
    "    rf_model = randomforest.fit(X_train, y_train)\n",
    "\n",
    "    preds = rf_model.predict(X_test)\n",
    "    score = rf_model.score(X_test, y_test)\n",
    "    \n",
    "#     print_confusion_matrix()\n",
    "    \n",
    "    time_elapsed = time.time() - time_start\n",
    "    return {'Score' : score, 'Time' : time_elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbors_classifier(X_train, y_train, X_test, y_test):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=5, n_jobs=2)\n",
    "    knn_model = knn.fit(X_train, y_train)\n",
    "    \n",
    "    preds = knn_model.predict(X_test)\n",
    "    score = knn_model.score(X_test, y_test)\n",
    "    \n",
    "#     print_confusion_matrix()\n",
    "    \n",
    "    time_elapsed = time.time() - time_start\n",
    "    return {'Score' : score, 'Time' : time_elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_classifier(X_train, y_train, X_test, y_test):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda_model = lda.fit(X_train, y_train)\n",
    "    \n",
    "    preds = lda_model.predict(X_test)\n",
    "    score = lda_model.score(X_test, y_test)\n",
    "    \n",
    "#     print_confusion_matrix()\n",
    "\n",
    "    time_elapsed = time.time() - time_start\n",
    "    return {'Score' : score, 'Time' : time_elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_all_classify(df, features_list, y_list):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    # Divide df by train and test devices\n",
    "    df_test = df[df[\"Set\"]==\"test\"]\n",
    "    df_train = df[df[\"Set\"]==\"train\"]\n",
    "    \n",
    "    # Train using chosen features\n",
    "    X_train = df_train[features_list]\n",
    "    X_test = df_test[features_list]\n",
    "\n",
    "    for device_type in y_list:\n",
    "        # Set one device type as y\n",
    "        y_train = df_train[device_type]\n",
    "        y_test = df_test[device_type]\n",
    "\n",
    "        time_start_clf = time.time()\n",
    "\n",
    "        rf_clf = random_forest_classifier(X_train, y_train, X_test, y_test)\n",
    "        knn_clf = k_neighbors_classifier(X_train, y_train, X_test, y_test)\n",
    "        lda_clf = lda_classifier(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        time_elapsed_clf = time.time() - time_start_clf\n",
    "\n",
    "        print \"Device Type:\", device_type\n",
    "        print \"Random Forest Score:\", rf_clf['Score'], \"Time: \", rf_clf['Time']\n",
    "        print \"KNN Score:\", knn_clf['Score'], \"Time: \", knn_clf['Time']\n",
    "        print \"LDA Score:\", lda_clf['Score'], \"Time: \", lda_clf['Time']\n",
    "        print \"Total time (classifiers):\", time_elapsed_clf\n",
    "        print \"\"\n",
    "    \n",
    "    print \"Total time (one vs all_classify):\", time.time() - time_start\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_one_classify(df, features_list, y_list):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    # Get possible combinations for one vs one\n",
    "    combinations = [combination for combination in itertools.combinations(y_list, 2)]\n",
    "\n",
    "    for device_pair in combinations:\n",
    "        # Only use data with the two device types needed for one vs one classification\n",
    "        pos_device_type = device_pair[0]\n",
    "        neg_device_type = device_pair[1]\n",
    "        df_1v1 = df[(df[\"DeviceType\"]==pos_device_type) | (df[\"DeviceType\"]==neg_device_type)]\n",
    "\n",
    "        # Separate df into train and test sets\n",
    "        df_train = df_1v1[df_1v1[\"Set\"]==\"train\"]\n",
    "        df_test = df_1v1[df_1v1[\"Set\"]==\"test\"]\n",
    "        X_train = df_train[features_list]\n",
    "        X_test = df_test[features_list]\n",
    "        y_train = df_train[pos_device_type]\n",
    "        y_test = df_test[pos_device_type]\n",
    "        \n",
    "        time_start_clf = time.time()\n",
    "\n",
    "        rf_clf = random_forest_classifier(X_train, y_train, X_test, y_test)\n",
    "        knn_clf = k_neighbors_classifier(X_train, y_train, X_test, y_test)\n",
    "        lda_clf = lda_classifier(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        time_elapsed_clf = time.time() - time_start_clf\n",
    "\n",
    "        print \"Device Pair:\", device_pair\n",
    "        print \"Random Forest Score:\", rf_clf['Score'], \"Time: \", rf_clf['Time']\n",
    "        print \"KNN Score:\", knn_clf['Score'], \"Time: \", knn_clf['Time']\n",
    "        print \"LDA Score:\", lda_clf['Score'], \"Time: \", lda_clf['Time']\n",
    "        print \"Total time (classifiers):\", time_elapsed_clf\n",
    "        print \"\"\n",
    "    \n",
    "    print \"Total time (one vs one_classify):\", time.time() - time_start\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old ./BLE_Source deleted\n",
      "Old ./BLE_Destination deleted\n",
      "Working packet # 0 ...\n",
      "Working packet # 25000 ...\n",
      "Working packet # 50000 ...\n",
      "Working packet # 75000 ...\n",
      "Working packet # 100000 ...\n",
      "Working packet # 125000 ...\n",
      "Working packet # 150000 ...\n",
      "Working packet # 175000 ...\n",
      "Working packet # 200000 ...\n",
      "Working packet # 225000 ...\n",
      "Working packet # 250000 ...\n",
      "Working packet # 275000 ...\n",
      "Working packet # 300000 ...\n",
      "Working packet # 325000 ...\n",
      "Working packet # 350000 ...\n",
      "Working packet # 375000 ...\n",
      "Working packet # 400000 ...\n",
      "Working packet # 425000 ...\n",
      "Working packet # 450000 ...\n",
      "Working packet # 475000 ...\n",
      "Working packet # 500000 ...\n",
      "Working packet # 525000 ...\n",
      "Working packet # 550000 ...\n",
      "Working packet # 575000 ...\n",
      "Working packet # 600000 ...\n",
      "Working packet # 625000 ...\n",
      "Working packet # 650000 ...\n",
      "Working packet # 675000 ...\n",
      "Working packet # 700000 ...\n",
      "Working packet # 725000 ...\n",
      "Working packet # 750000 ...\n",
      "Working packet # 775000 ...\n",
      "Working packet # 800000 ...\n",
      "Working packet # 825000 ...\n",
      "Working packet # 850000 ...\n",
      "Working packet # 875000 ...\n",
      "Working packet # 900000 ...\n",
      "Working packet # 925000 ...\n",
      "Working packet # 950000 ...\n",
      "Working packet # 975000 ...\n",
      "Working packet # 1000000 ...\n",
      "Working packet # 1025000 ...\n",
      "Working packet # 1050000 ...\n",
      "Working packet # 1075000 ...\n",
      "Working packet # 1100000 ...\n",
      "Working packet # 1125000 ...\n",
      "Working packet # 1150000 ...\n",
      "Working packet # 1175000 ...\n",
      "Working packet # 1200000 ...\n",
      "Working packet # 1225000 ...\n",
      "Working packet # 1250000 ...\n",
      "Working packet # 1275000 ...\n",
      "Working packet # 1300000 ...\n",
      "Working packet # 1325000 ...\n",
      "Working packet # 1350000 ...\n",
      "Working packet # 1375000 ...\n",
      "Working packet # 1400000 ...\n",
      "Working packet # 1425000 ...\n",
      "Working packet # 1450000 ...\n",
      "Working packet # 1475000 ...\n",
      "Working packet # 1500000 ...\n",
      "Working packet # 1525000 ...\n",
      "Working packet # 1550000 ...\n",
      "Working packet # 1575000 ...\n",
      "Working packet # 1600000 ...\n",
      "Working packet # 1625000 ...\n",
      "Working packet # 1650000 ...\n",
      "Working packet # 1675000 ...\n",
      "Working packet # 1700000 ...\n",
      "Working packet # 1725000 ...\n",
      "Working packet # 1750000 ...\n",
      "Working packet # 1775000 ...\n",
      "Working packet # 1800000 ...\n",
      "Working packet # 1825000 ...\n",
      "Working packet # 1850000 ...\n",
      "Working packet # 1875000 ...\n",
      "Working packet # 1900000 ...\n",
      "Working packet # 1925000 ...\n",
      "Working packet # 1950000 ...\n",
      "Working packet # 1975000 ...\n",
      "Working packet # 2000000 ...\n",
      "Working packet # 2025000 ...\n",
      "Working packet # 2050000 ...\n",
      "Working packet # 2075000 ...\n",
      "Working packet # 2100000 ...\n",
      "Working packet # 2125000 ...\n",
      "Working packet # 2150000 ...\n",
      "Working packet # 2175000 ...\n",
      "Working packet # 2200000 ...\n",
      "Working packet # 2225000 ...\n",
      "Working packet # 2250000 ...\n",
      "Working packet # 2275000 ...\n",
      "Working packet # 2300000 ...\n",
      "Working packet # 2325000 ...\n",
      "Working packet # 2350000 ...\n",
      "Working packet # 2375000 ...\n",
      "Working packet # 2400000 ...\n",
      "Working packet # 2425000 ...\n",
      "Working packet # 2450000 ...\n",
      "Working packet # 2475000 ...\n",
      "Working packet # 2500000 ...\n",
      "Working packet # 2525000 ...\n",
      "Working packet # 2550000 ...\n",
      "Working packet # 2575000 ...\n",
      "Working packet # 2600000 ...\n",
      "Working packet # 2625000 ...\n",
      "Working packet # 2650000 ...\n",
      "Working packet # 2675000 ...\n",
      "Working packet # 2700000 ...\n",
      "Working packet # 2725000 ...\n",
      "Working packet # 2750000 ...\n",
      "Working packet # 2775000 ...\n",
      "Working packet # 2800000 ...\n",
      "Working packet # 2825000 ...\n",
      "Working packet # 2850000 ...\n",
      "Working packet # 2875000 ...\n",
      "Working packet # 2900000 ...\n",
      "Working packet # 2925000 ...\n",
      "Working packet # 2950000 ...\n",
      "Working packet # 2975000 ...\n",
      "Working packet # 3000000 ...\n",
      "Working packet # 3025000 ...\n",
      "Working packet # 3050000 ...\n",
      "Working packet # 3075000 ...\n",
      "Working packet # 3100000 ...\n",
      "Working packet # 3125000 ...\n",
      "Working packet # 3150000 ...\n",
      "Working packet # 3175000 ...\n",
      "Working packet # 3200000 ...\n",
      "Working packet # 3225000 ...\n",
      "Working packet # 3250000 ...\n",
      "Working packet # 3275000 ...\n",
      "Working packet # 3300000 ...\n",
      "Working packet # 3325000 ...\n",
      "Working packet # 3350000 ...\n",
      "Working packet # 3375000 ...\n",
      "Working packet # 3400000 ...\n",
      "Working packet # 3425000 ...\n",
      "Working packet # 3450000 ...\n",
      "Working packet # 3475000 ...\n",
      "Working packet # 3500000 ...\n",
      "Working packet # 3525000 ...\n",
      "Working packet # 3550000 ...\n",
      "Working packet # 3575000 ...\n",
      "Working packet # 3600000 ...\n",
      "Working packet # 3625000 ...\n",
      "Working packet # 3650000 ...\n",
      "Working packet # 3675000 ...\n",
      "Working packet # 3700000 ...\n",
      "Working packet # 3725000 ...\n",
      "Working packet # 3750000 ...\n",
      "Working packet # 3775000 ...\n",
      "Working packet # 3800000 ...\n",
      "Working packet # 3825000 ...\n",
      "Working packet # 3850000 ...\n",
      "Working packet # 3875000 ...\n",
      "Working packet # 3900000 ...\n",
      "Working packet # 3925000 ...\n",
      "Working packet # 3950000 ...\n",
      "Working packet # 3975000 ...\n",
      "Working packet # 4000000 ...\n",
      "Working packet # 4025000 ...\n",
      "Working packet # 4050000 ...\n",
      "Working packet # 4075000 ...\n",
      "Working packet # 4100000 ...\n",
      "Working packet # 4125000 ...\n",
      "Working packet # 4150000 ...\n",
      "Working packet # 4175000 ...\n",
      "Working packet # 4200000 ...\n",
      "Working packet # 4225000 ...\n",
      "Working packet # 4250000 ...\n",
      "Working packet # 4275000 ...\n",
      "Total number of packets processed:  4285634\n",
      "Total data processing time:  4658.09532809\n",
      "Normalized total processing time per 25k packets:  27.172731783\n",
      "Total capture file processing time:  943.022395611\n",
      "Normalized capture file processing time:  5.50106702772\n"
     ]
    }
   ],
   "source": [
    "# ble_extract_packet_features(filename='/root/Documents/Thesis/BLE_PCAPS/home1home2-15min.pcap', create_master=False)\n",
    "ble_extract_packet_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "August2    224739\n",
      "Energy1     79039\n",
      "Energy2     71741\n",
      "Home1       58810\n",
      "Home2       58614\n",
      "Push        32761\n",
      "Kevo        21107\n",
      "August1     17314\n",
      "Weather      8643\n",
      "Room2        8133\n",
      "Room1        7728\n",
      "Door1        7374\n",
      "Door2        4154\n",
      "Name: Name, dtype: int64\n",
      "Time for dataframe: 537.221572161\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "df = make_dataframe()\n",
    "print \"Time for dataframe:\", time.time() - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs all\n",
      "Device Type: door\n",
      "Random Forest Score: 0.03878899361251508 Time:  1.25428891182\n",
      "KNN Score: 0.9743994329735979 Time:  103.754995108\n",
      "LDA Score: 0.03437597563136534 Time:  0.340323925018\n",
      "Total time (classifiers): 105.349692822\n",
      "\n",
      "Device Type: lock\n",
      "Random Forest Score: 0.2745184072633382 Time:  1.04208707809\n",
      "KNN Score: 0.9978441183667615 Time:  103.307614088\n",
      "LDA Score: 0.05184663285884249 Time:  0.339652061462\n",
      "Total time (classifiers): 104.689419031\n",
      "\n",
      "Device Type: plug\n",
      "Random Forest Score: 0.9999746863213318 Time:  0.951542139053\n",
      "KNN Score: 0.9977470825985335 Time:  103.337182999\n",
      "LDA Score: 0.0950655202382861 Time:  0.349932193756\n",
      "Total time (classifiers): 104.638740063\n",
      "\n",
      "Device Type: temp\n",
      "Random Forest Score: 0.9786225983647363 Time:  1.04567193985\n",
      "KNN Score: 0.9750491507260807 Time:  103.189161062\n",
      "LDA Score: 0.9656915275117498 Time:  0.332150936127\n",
      "Total time (classifiers): 104.567066908\n",
      "\n",
      "Total time (one vs all_classify): 419.3985641\n",
      "\n",
      "One vs one\n",
      "Device Pair: ('door', 'lock')\n",
      "Random Forest Score: 0.9975490731477153 Time:  0.493432044983\n",
      "KNN Score: 0.9975578108548536 Time:  22.3428668976\n",
      "LDA Score: 0.018143848872617337 Time:  0.161269903183\n",
      "Total time (classifiers): 22.9976229668\n",
      "\n",
      "Device Pair: ('door', 'plug')\n",
      "Random Forest Score: 0.9992778045257583 Time:  0.811378955841\n",
      "KNN Score: 0.7951372171401059 Time:  98.6185040474\n",
      "LDA Score: 0.00024073182474723158 Time:  0.244150161743\n",
      "Total time (classifiers): 99.6741220951\n",
      "\n",
      "Device Pair: ('door', 'temp')\n",
      "Random Forest Score: 0.5621388459347277 Time:  0.63910484314\n",
      "KNN Score: 0.5360950598193213 Time:  13.150676012\n",
      "LDA Score: 0.6620818751526003 Time:  0.135388851166\n",
      "Total time (classifiers): 13.9252500534\n",
      "\n",
      "Device Pair: ('lock', 'plug')\n",
      "Random Forest Score: 0.9998487133964288 Time:  0.516603946686\n",
      "KNN Score: 0.9989365441690138 Time:  44.1596331596\n",
      "LDA Score: 0.9998220157605044 Time:  0.183835029602\n",
      "Total time (classifiers): 44.860147953\n",
      "\n",
      "Device Pair: ('lock', 'temp')\n",
      "Random Forest Score: 0.9978700745473909 Time:  0.372524023056\n",
      "KNN Score: 0.9980203717063452 Time:  8.88442707062\n",
      "LDA Score: 0.9998239376137964 Time:  0.095253944397\n",
      "Total time (classifiers): 9.35225892067\n",
      "\n",
      "Device Pair: ('plug', 'temp')\n",
      "Random Forest Score: 0.9997540882823066 Time:  0.615221977234\n",
      "KNN Score: 0.9165129718431083 Time:  60.3537781239\n",
      "LDA Score: 0.9971720152465265 Time:  0.163506031036\n",
      "Total time (classifiers): 61.1325769424\n",
      "\n",
      "Total time (one vs one_classify): 253.041950941\n",
      "\n",
      "Total time (one vs one & one vs all classification): 672.458309889\n"
     ]
    }
   ],
   "source": [
    "# Run One vs All  and One vs One classification strategies\n",
    "features_list = [\n",
    "#     'AccessAddr', 'AdvertAddr', 'ScanAddr',\n",
    "    'BLE_LL_Length', 'TxAddr', 'CompanyID',\n",
    "#     'RFChannel',\n",
    "    'PacketLength', 'Time', 'Assoc_Packets',\n",
    "    'ADV_DIRECT_IND', 'ADV_IND', 'ADV_NONCONN_IND', \n",
    "    'ADV_SCAN_IND', 'CONNECT_REQ', 'SCAN_REQ', 'SCAN_RSP']\n",
    "\n",
    "y_list = [\"door\", \"lock\", \"plug\", \"temp\"]\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "print \"One vs all\"\n",
    "one_vs_all_classify(df, features_list, y_list)\n",
    "\n",
    "print \"One vs one\"\n",
    "one_vs_one_classify(df, features_list, y_list)\n",
    "\n",
    "print \"Total time (one vs one & one vs all classification):\", time.time() - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs all\n",
      "Device Type: door\n",
      "Random Forest Score: 0.03878899361251508 Time:  1.13068795204\n",
      "KNN Score: 0.9743994329735979 Time:  103.12498188\n",
      "LDA Score: 0.03437597563136534 Time:  0.338510036469\n",
      "Total time (classifiers): 104.594233036\n",
      "\n",
      "Device Type: lock\n",
      "Random Forest Score: 0.2745184072633382 Time:  0.894592046738\n",
      "KNN Score: 0.9978441183667615 Time:  102.940897226\n",
      "LDA Score: 0.05184663285884249 Time:  0.336610794067\n",
      "Total time (classifiers): 104.172151089\n",
      "\n",
      "Device Type: temp\n",
      "Random Forest Score: 0.9786225983647363 Time:  1.03509902954\n",
      "KNN Score: 0.9750491507260807 Time:  103.008431911\n",
      "LDA Score: 0.9656915275117498 Time:  0.334670066833\n",
      "Total time (classifiers): 104.37825489\n",
      "\n",
      "Total time (one vs all_classify): 313.296242952\n",
      "\n",
      "One vs one\n",
      "Device Pair: ('door', 'lock')\n",
      "Random Forest Score: 0.9975490731477153 Time:  0.4849858284\n",
      "KNN Score: 0.9975578108548536 Time:  22.4209668636\n",
      "LDA Score: 0.018143848872617337 Time:  0.160078048706\n",
      "Total time (classifiers): 23.0660808086\n",
      "\n",
      "Device Pair: ('door', 'temp')\n",
      "Random Forest Score: 0.5621388459347277 Time:  0.625123023987\n",
      "KNN Score: 0.5360950598193213 Time:  13.1909298897\n",
      "LDA Score: 0.6620818751526003 Time:  0.135900974274\n",
      "Total time (classifiers): 13.9520578384\n",
      "\n",
      "Device Pair: ('lock', 'temp')\n",
      "Random Forest Score: 0.9978700745473909 Time:  0.365028858185\n",
      "KNN Score: 0.9980203717063452 Time:  8.83682799339\n",
      "LDA Score: 0.9998239376137964 Time:  0.0962378978729\n",
      "Total time (classifiers): 9.29814600945\n",
      "\n",
      "Total time (one vs one_classify): 46.8553628922\n",
      "\n",
      "Total time (one vs one & one vs all classification): 360.17505002\n"
     ]
    }
   ],
   "source": [
    "# Run One vs All  and One vs One classification strategies\n",
    "features_list = [\n",
    "#     'AccessAddr', 'AdvertAddr', 'ScanAddr',\n",
    "    'BLE_LL_Length', 'TxAddr', 'CompanyID',\n",
    "#     'RFChannel',\n",
    "    'PacketLength', 'Time', 'Assoc_Packets',\n",
    "    'ADV_DIRECT_IND', 'ADV_IND', 'ADV_NONCONN_IND', \n",
    "    'ADV_SCAN_IND', 'CONNECT_REQ', 'SCAN_REQ', 'SCAN_RSP']\n",
    "\n",
    "y_list = [\"door\", \"lock\", \"temp\"]\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "print \"One vs all\"\n",
    "one_vs_all_classify(df, features_list, y_list)\n",
    "\n",
    "print \"One vs one\"\n",
    "one_vs_one_classify(df, features_list, y_list)\n",
    "\n",
    "print \"Total time (one vs one & one vs all classification):\", time.time() - time_start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
