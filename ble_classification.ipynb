{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pipeline import BLEPipeline, WifiPipeline\n",
    "\n",
    "# General data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "\n",
    "# ML libraries\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# System libraries\n",
    "from itertools import izip, combinations\n",
    "import random, time\n",
    "\n",
    "# Warning filtering\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BLEPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.extract_packet_features(create_master=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "August2    210644\n",
      "Home1       54902\n",
      "Home2       54516\n",
      "Push        30661\n",
      "Kevo        19430\n",
      "August1     15047\n",
      "Weather      8101\n",
      "Room2        7698\n",
      "Room1        7239\n",
      "Door1        6696\n",
      "Door2        3587\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = b.make_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\n",
    "    # Packet info\n",
    "    \"PacketLength\", \"BLE_LL_Length\", \"Time\", \n",
    "    \n",
    "    # Associate Packets\n",
    "    \"Assoc_Packets\",\n",
    "    \n",
    "    # Channel number\n",
    "    \"Channel_0\", \"Channel_12\", \"Channel_39\",\n",
    "    \n",
    "    # PDU Type\n",
    "    \"SCAN_RSP\", \"ADV_IND\", \"SCAN_REQ\", \n",
    "    \"CONNECT_REQ\", \"ADV_NONCONN_IND\", \"ADV_DIRECT_IND\"]\n",
    "\n",
    "y_list = [\"door\", \"lock\", \"temp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove router device type and plug device type (not using plug because not enough devices)\n",
    "df = df[(df[\"DeviceType\"]!=\"router\") & (df[\"DeviceType\"]!=\"plug\")]\n",
    "\n",
    "df_train = df[df['Set']=='train']\n",
    "\n",
    "# List variables that do not work with resampling methods\n",
    "dataframe_vars = {'Name', 'DeviceName', 'Set', \n",
    "                  'AccessAddr', 'AdvertAddr','PDUTypeNum'}\n",
    "onehotEncoded_vars = {'ADV_DIRECT_IND', 'ADV_IND', \n",
    "                      'ADV_NONCONN_IND', 'ADV_SCAN_IND', \n",
    "                      'CONNECT_REQ', 'SCAN_REQ', 'SCAN_RSP'}\n",
    "response_vars = {\"DeviceType\",\"door\",\"lock\",\"temp\"}\n",
    "ble_devicetypes = [\"door\",\"lock\",\"temp\"]\n",
    "\n",
    "# Prep X, y\n",
    "y = df_train[\"DeviceType\"]\n",
    "col_drop = response_vars | dataframe_vars | onehotEncoded_vars\n",
    "X = df_train.drop(col_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create trial dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = []\n",
    "num_trials = 10\n",
    "while len(df_trials) != num_trials:\n",
    "    df_downsampled = b.downsample(X, y, df[df[\"Set\"]==\"test\"])\n",
    "\n",
    "    # Ensure that the trial sample contains all features in feature list\n",
    "    if set(features_list).issubset(set(df_downsampled.columns)):\n",
    "        df_trials.append(df_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "Trial 10\n"
     ]
    }
   ],
   "source": [
    "trial_results = []\n",
    "for i, trial in enumerate(df_trials):\n",
    "    print \"Trial\", i+1\n",
    "    result = b.one_vs_all_classify(trial, [x for x in features_list if x in trial.columns], y_list)\n",
    "    trial_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alloutput = b.store_trial_results(trial_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.create_csv_results('ble', trial_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>SD_Accuracy</th>\n",
       "      <th>CI_Accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>SD_TPR</th>\n",
       "      <th>CI_TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>SD_FPR</th>\n",
       "      <th>CI_FPR</th>\n",
       "      <th>FNR</th>\n",
       "      <th>SD_FNR</th>\n",
       "      <th>CI_FNR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>SD_Precision</th>\n",
       "      <th>CI_Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Accuracy, SD_Accuracy, CI_Accuracy, TPR, SD_TPR, CI_TPR, FPR, SD_FPR, CI_FPR, FNR, SD_FNR, CI_FNR, Precision, SD_Precision, CI_Precision]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_performance_device = b.report_metrics_across('Device', df_alloutput)\n",
    "display(df_performance_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>SD_Accuracy</th>\n",
       "      <th>CI_Accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>SD_TPR</th>\n",
       "      <th>CI_TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>SD_FPR</th>\n",
       "      <th>CI_FPR</th>\n",
       "      <th>FNR</th>\n",
       "      <th>SD_FNR</th>\n",
       "      <th>CI_FNR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>SD_Precision</th>\n",
       "      <th>CI_Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Accuracy, SD_Accuracy, CI_Accuracy, TPR, SD_TPR, CI_TPR, FPR, SD_FPR, CI_FPR, FNR, SD_FNR, CI_FNR, Precision, SD_Precision, CI_Precision]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_performance_classifier = b.report_metrics_across('Classifier', df_alloutput)\n",
    "display(df_performance_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC plots, report AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-24e30d7f7ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrial_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Extract devices and classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdev_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting ROC curves\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "for trial_num, result in enumerate(df_results):\n",
    "    # Extract devices and classifiers\n",
    "    dev_pairs = result[0].keys()\n",
    "    classifiers = result[0][dev_pairs[0]].keys()\n",
    "    \n",
    "    for device in dev_pairs:\n",
    "        for classifier in classifiers:\n",
    "            # Extract predicted probas and y_true\n",
    "            pred_proba = result[0][device][classifier]['Classifier']['Pred_Proba']\n",
    "            true = result[0][device][classifier]['Classifier']['True']\n",
    "\n",
    "            # Plot ROC curve\n",
    "            fpr, tpr, threshold = roc_curve(true, pred_proba[:,1], pos_label=1)\n",
    "            auc = roc_auc_score(true, pred_proba[:,1])\n",
    "            title = \"Trial \" + str(trial_num + 1) + \": \" + str(device).capitalize() + \" vs Rest -- \" + str(classifier).upper() \n",
    "            skplt.metrics.plot_roc(true, pred_proba, title=title, plot_micro=False, plot_macro=False, \n",
    "                                   classes_to_plot=[1], figsize=(8,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trial_num, result in enumerate(df_results):\n",
    "    print \"Trial \", trial_num\n",
    "    print \"Total time:\", result[1], \"seconds\"\n",
    "    \n",
    "    for device in y_list:\n",
    "        for classifier in classifiers:\n",
    "            title = \"(\"+ str(device).capitalize() + \", \"+str(classifier).upper() + \")\"\n",
    "            print title, \":\", result[0][device][classifier]['Classifier']['Time'], \"seconds\"\n",
    "    \n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featImp = pd.DataFrame()\n",
    "for i,f in enumerate(b.feature_importances):\n",
    "    f_i = pd.Series(f, index=features_list)\n",
    "    df_featImp[i] = f_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, scipy.stats as st\n",
    "a = df_featImp.iloc[:,0:30].T\n",
    "lower, upper = st.t.interval(0.90, len(a)-1, loc=np.mean(a), scale=st.sem(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featImp['Mean'] = df_featImp.mean(axis=1)\n",
    "df_featImp['StdDev'] = df_featImp.std(axis=1)\n",
    "df_featImp['CI_Lower'] = lower\n",
    "df_featImp['CI_Upper'] = upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_featImp[['Mean', 'StdDev', 'CI_Lower', 'CI_Upper']].sort_values('Mean', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample packets based on DeviceType\n",
    "# Useful info used from https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets#\n",
    "devtype_df = df[\"DeviceType\"].value_counts().sort_index()\n",
    "devtype_df.plot(kind='bar', title=\"Packet Counts Prior to Resampling\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 2, figsize=(14,4)) # number of rows, number of columns, figure size=(width, height)\n",
    "\n",
    "# Plot training set\n",
    "df_train = df[df[\"Set\"]==\"train\"]\n",
    "plt.subplot(1, 2, 1)\n",
    "df_train[\"DeviceType\"].value_counts().sort_index().plot(kind='bar', title=\"Training Packet Counts Prior to Resampling\");\n",
    "\n",
    "# Plot test set\n",
    "df_test = df[df[\"Set\"]==\"test\"]\n",
    "plt.subplot(1, 2, 2)\n",
    "df_test[\"DeviceType\"].value_counts().sort_index().plot(kind='bar', title=\"Test Packet Counts Prior to Resampling\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 2, figsize=(14,4)) # number of rows, number of columns, figure size=(width, height)\n",
    "\n",
    "df_train_downsampled = df_trials[0][df_trials[0]['Set']=='train']\n",
    "df_test_downsampled = df_trials[0][df_trials[0]['Set']=='test']\n",
    "\n",
    "# Plot training set\n",
    "df_train_downsampled['DeviceType'] = df_train_downsampled[ble_devicetypes].idxmax(1)\n",
    "plt.subplot(1, 2, 1)\n",
    "df_train_downsampled['DeviceType'].value_counts().sort_index().plot(kind='bar', title=\"Training Packet Counts After Resampling\");\n",
    "\n",
    "# Plot test set\n",
    "df_test = df[df[\"Set\"]==\"test\"]\n",
    "plt.subplot(1, 2, 2)\n",
    "df_test[\"DeviceType\"].value_counts().sort_index().plot(kind='bar', title=\"Test Packet Counts After Resampling\");\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
